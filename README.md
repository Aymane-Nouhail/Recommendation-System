# Hybrid Variational Autoencoder for Recommendation# HybridVAE Recommendation System# HybridVAE Recommendation System# Hybrid VAE Recommendation System



This repository contains an implementation of a hybrid recommendation system that combines collaborative filtering with pre-trained text embeddings. The model uses a Variational Autoencoder (VAE) architecture where item embeddings from Sentence-BERT serve as a frozen decoder, enabling the system to leverage both user-item interaction patterns and semantic item representations.



The project includes a fully autonomous pipeline that handles the entire workflow from raw data to evaluation results with a single command.A state-of-the-art recommendation system using a Hybrid Variational Autoencoder that combines collaborative filtering with semantic item embeddings from SBERT. Evaluated on Amazon product review datasets.



---



## Table of Contents## ğŸ¯ Key ResultsA state-of-the-art recommendation system using a Hybrid Variational Autoencoder that combines collaborative filtering with semantic item embeddings from SBERT. Evaluated on Amazon product review datasets.A sophisticated recommendation system using a Hybrid Variational Autoencoder (VAE) that combines collaborative filtering with item text embeddings from SBERT.



- [Results](#results)

- [Model Architecture](#model-architecture)

- [Autonomous Pipeline](#autonomous-pipeline)### Performance Comparison (NDCG@10)

- [Implementation Details](#implementation-details)

- [Installation](#installation)

- [Usage](#usage)

- [Methodology](#methodology)| Model | All_Beauty | Appliances |## ğŸ¯ Key Results## Project Structure

- [Project Structure](#project-structure)

- [References](#references)|-------|------------|------------|



---| **HybridVAE (Ours)** | **0.213** | 0.663 |



## Results| LightGCN | 0.189 | **0.668** |



The model was evaluated on two Amazon product review datasets with different characteristics.| SVD | 0.189 | 0.373 |### Performance Comparison (NDCG@10)```



### Summary (NDCG@10)| Mult-VAE | 0.178 | 0.529 |



| Model | All_Beauty | Appliances || Item-KNN | 0.152 | 0.317 |recommendation_system/

|:------|:----------:|:----------:|

| **HybridVAE** | **0.213** | 0.663 || Popularity | 0.104 | 0.185 |

| LightGCN | 0.189 | **0.668** |

| SVD | 0.189 | 0.373 || Model | All_Beauty | Appliances |â”œâ”€â”€ data/                    # Dataset storage

| Mult-VAE | 0.178 | 0.529 |

| Item-KNN | 0.152 | 0.317 |**Key Findings:**

| Popularity | 0.104 | 0.185 |

- HybridVAE outperforms LightGCN by **12.7%** on All_Beauty (sparse dataset)|-------|------------|------------|â”œâ”€â”€ models/                  # Saved model checkpoints

On the sparse All_Beauty dataset (22,363 users, 12,101 items), HybridVAE outperforms LightGCN by 12.7%. On the denser Appliances dataset (2,072 users, 890 items), the two models perform comparably.

- Competitive with LightGCN on Appliances (dense dataset)

### Baseline Comparison

- Significantly outperforms traditional methods (SVD, Item-KNN, Popularity)| **HybridVAE (Ours)** | **0.213** | 0.663 |â”œâ”€â”€ embeddings/             # Pre-computed item embeddings

<p align="center">

  <img src="assets/all_beauty_baseline.png" width="45%" />

  &nbsp;&nbsp;

  <img src="assets/appliances_baseline.png" width="45%" />### Baseline Comparison| LightGCN | 0.189 | **0.668** |â”œâ”€â”€ src/                    # Source code

</p>



<p align="center">

  <sub>Left: All_Beauty dataset. Right: Appliances dataset.</sub><p align="center">| SVD | 0.189 | 0.373 |â”‚   â”œâ”€â”€ preprocessing/      # Data processing modules

</p>

  <img src="assets/all_beauty_baseline.png" width="48%" alt="All_Beauty Baseline Comparison"/>

### Detailed Results

  <img src="assets/appliances_baseline.png" width="48%" alt="Appliances Baseline Comparison"/>| Mult-VAE | 0.178 | 0.529 |â”‚   â”‚   â”œâ”€â”€ cleaning.py     # Data loading and cleaning

<details>

<summary>All_Beauty Dataset</summary></p>



| Model | Recall@5 | Recall@10 | Recall@20 | NDCG@5 | NDCG@10 | NDCG@20 |<p align="center"><em>Left: All_Beauty dataset | Right: Appliances dataset</em></p>| Item-KNN | 0.152 | 0.317 |â”‚   â”‚   â”œâ”€â”€ dataset.py      # Dataset construction

|:------|:--------:|:---------:|:---------:|:------:|:-------:|:-------:|

| HybridVAE | 0.195 | 0.286 | 0.403 | 0.181 | 0.213 | 0.247 |

| LightGCN | 0.161 | 0.240 | 0.353 | 0.158 | 0.189 | 0.224 |

| SVD | 0.160 | 0.243 | 0.365 | 0.156 | 0.189 | 0.227 |### Full Results| Popularity | 0.104 | 0.185 |â”‚   â”‚   â””â”€â”€ embeddings.py   # SBERT text embeddings

| Mult-VAE | 0.149 | 0.227 | 0.342 | 0.145 | 0.178 | 0.214 |

| Item-KNN | 0.117 | 0.189 | 0.299 | 0.120 | 0.152 | 0.189 |

| Popularity | 0.076 | 0.131 | 0.219 | 0.080 | 0.104 | 0.134 |

#### All_Beauty Dataset (22,363 users, 12,101 items)â”‚   â”œâ”€â”€ ml/                 # Machine learning modules

</details>

| Model | Recall@5 | Recall@10 | Recall@20 | NDCG@5 | NDCG@10 | NDCG@20 |

<details>

<summary>Appliances Dataset</summary>|-------|----------|-----------|-----------|--------|---------|---------|### Full Resultsâ”‚   â”‚   â”œâ”€â”€ model.py        # Hybrid VAE implementation



| Model | Recall@5 | Recall@10 | Recall@20 | NDCG@5 | NDCG@10 | NDCG@20 || **HybridVAE** | 0.195 | **0.286** | 0.403 | **0.181** | **0.213** | 0.247 |

|:------|:--------:|:---------:|:---------:|:------:|:-------:|:-------:|

| HybridVAE | 0.603 | 0.711 | 0.810 | 0.600 | 0.663 | 0.688 || LightGCN | 0.161 | 0.240 | 0.353 | 0.158 | 0.189 | 0.224 |â”‚   â”‚   â”œâ”€â”€ train.py        # Training loop

| LightGCN | 0.617 | 0.724 | 0.820 | 0.605 | 0.668 | 0.694 |

| SVD | 0.386 | 0.482 | 0.609 | 0.341 | 0.373 | 0.413 || SVD | 0.160 | 0.243 | 0.365 | 0.156 | 0.189 | 0.227 |

| Mult-VAE | 0.471 | 0.583 | 0.704 | 0.478 | 0.529 | 0.563 |

| Item-KNN | 0.265 | 0.367 | 0.502 | 0.267 | 0.317 | 0.362 |#### All_Beauty Dataset (22,363 users, 12,101 items)â”‚   â”‚   â””â”€â”€ evaluate.py     # Evaluation metrics

| Popularity | 0.149 | 0.224 | 0.342 | 0.147 | 0.185 | 0.226 |

#### Appliances Dataset (2,072 users, 890 items)

</details>

| Model | Recall@5 | Recall@10 | Recall@20 | NDCG@5 | NDCG@10 | NDCG@20 || Model | Recall@5 | Recall@10 | Recall@20 | NDCG@5 | NDCG@10 | NDCG@20 |â”‚   â”œâ”€â”€ api/                # API modules

---

|-------|----------|-----------|-----------|--------|---------|---------|

## Model Architecture

| **HybridVAE** | 0.603 | 0.711 | 0.810 | 0.600 | 0.663 | 0.688 ||-------|----------|-----------|-----------|--------|---------|---------|â”‚   â”‚   â”œâ”€â”€ server.py       # FastAPI server

The HybridVAE combines a standard VAE encoder with a decoder that uses pre-computed Sentence-BERT embeddings as fixed weights.

| LightGCN | 0.617 | 0.724 | 0.820 | 0.605 | 0.668 | 0.694 |

```

Input: User interaction vector x âˆˆ {0,1}^n_items| SVD | 0.386 | 0.482 | 0.609 | 0.341 | 0.373 | 0.413 || **HybridVAE** | 0.195 | **0.286** | 0.403 | **0.181** | **0.213** | 0.247 |â”‚   â”‚   â””â”€â”€ schemas.py      # Pydantic models

                    â”‚

                    â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚ Encoder                                       â”‚## ğŸ—ï¸ Architecture| LightGCN | 0.161 | 0.240 | 0.353 | 0.158 | 0.189 | 0.224 |â”‚   â””â”€â”€ utils.py           # Helper functions

â”‚   Linear(n_items â†’ hidden_dim)                â”‚

â”‚   LayerNorm â†’ GELU â†’ Dropout                  â”‚

â”‚   Linear(hidden_dim â†’ latent_dim) â†’ Î¼         â”‚

â”‚   Linear(hidden_dim â†’ latent_dim) â†’ log(ÏƒÂ²)   â”‚### HybridVAE Model| SVD | 0.160 | 0.243 | 0.365 | 0.156 | 0.189 | 0.227 |â”œâ”€â”€ notebooks/              # Jupyter notebooks

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                    â”‚

                    â–¼ z = Î¼ + Ïƒ âŠ™ Îµ,  Îµ ~ N(0, I)

                    â”‚```â”œâ”€â”€ requirements.txt        # Python dependencies

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚ Decoder                                       â”‚Input: User interaction vector (n_items,)

â”‚   Projection: Linear(latent_dim â†’ emb_dim)    â”‚

â”‚   Scores: E Â· z_projected  (E is frozen)      â”‚         â”‚#### Appliances Dataset (2,072 users, 890 items)â””â”€â”€ README.md              # This file

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                    â”‚         â–¼

                    â–¼

Output: Reconstruction scores âˆˆ R^n_itemsâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”| Model | Recall@5 | Recall@10 | Recall@20 | NDCG@5 | NDCG@10 | NDCG@20 |```

```

â”‚  Encoder                            â”‚

The item embedding matrix E is computed once using Sentence-BERT (`all-MiniLM-L6-v2`, 384 dimensions) on concatenated item titles and review text. These embeddings remain frozen during training, which serves as a regularization mechanism and enables the model to generalize to items with limited interaction data.

â”‚  Linear(n_items â†’ 512) + LayerNorm  â”‚|-------|----------|-----------|-----------|--------|---------|---------|

**Loss function:**

â”‚  GELU + Dropout(0.3)                â”‚

```

L = L_recon + Î² Â· D_KL(q(z|x) || p(z))â”‚  Linear(512 â†’ Î¼, Ïƒ)                 â”‚| **HybridVAE** | 0.603 | 0.711 | 0.810 | 0.600 | 0.663 | 0.688 |## Installation

```

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

where L_recon is the multinomial negative log-likelihood and Î² is annealed from 0 to 0.2 over training.

         â”‚| LightGCN | 0.617 | 0.724 | 0.820 | 0.605 | 0.668 | 0.694 |

---

         â–¼ Reparameterization: z = Î¼ + ÏƒÂ·Îµ

## Autonomous Pipeline

         â”‚| SVD | 0.386 | 0.482 | 0.609 | 0.341 | 0.373 | 0.413 |1. Clone the repository and navigate to the project directory:

A key feature of this project is the fully automated pipeline that executes the entire experimental workflow with a single command. The pipeline is implemented via a Makefile and handles:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

1. **Data preprocessing** â€” cleaning, filtering, and temporal train/validation/test splitting

2. **Embedding computation** â€” generating Sentence-BERT embeddings for all itemsâ”‚  Decoder (Frozen SBERT Embeddings)  â”‚```bash

3. **Hyperparameter tuning** â€” grid search over latent dimensions, hidden dimensions, dropout, and Î²

4. **Model training** â€” training with the best configuration found during tuningâ”‚  Projection: Linear(latent â†’ 384)   â”‚

5. **Evaluation** â€” computing Recall@K, NDCG@K, and Hit Ratio@K on the test set

6. **Baseline comparison** â€” running Popularity, Item-KNN, SVD, Mult-VAE, and LightGCNâ”‚  logits = Embeddings @ z            â”‚**Key Findings:**cd recommendation_system

7. **Visualization** â€” generating training curves, baseline comparisons, and latent space plots

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

To run the complete pipeline:

         â”‚- HybridVAE outperforms LightGCN by **12.7%** on All_Beauty (sparse dataset)```

```bash

make all         â–¼

```

Output: Reconstruction scores (n_items,)- Competitive with LightGCN on Appliances (dense dataset)

The pipeline reads the dataset configuration from `.env` and automatically propagates the best hyperparameters from tuning to training. Results are saved to JSON files for reproducibility, and all generated plots are stored in `models/figures/`.

```

### Pipeline Implementation

- Significantly outperforms traditional methods (SVD, Item-KNN, Popularity)2. Install dependencies:

The `train-best` target demonstrates how the pipeline chains steps together:

**Key Components:**

```makefile

train-best:- **SBERT Embeddings**: `all-MiniLM-L6-v2` (384 dimensions) - frozen during training```bash

    @# Extract best config from grid search results

    @$(PYTHON) -c "\- **Latent Space**: 128 dimensions (found via grid search)

    import json; \

    cfg = json.load(open('models/grid_search_results.json'))['best_config']; \- **Loss**: Reconstruction (BCE) + Î² Ã— KL Divergence with annealing## ğŸ—ï¸ Architecturepip install -r requirements.txt

    ..." 

    @# Train with extracted hyperparameters- **Regularization**: Dropout, LayerNorm, gradient clipping

    $(PYTHON) src/ml/train.py --latent-dim $(LATENT) --hidden-dims $(HIDDEN) ...

``````



This design ensures that:## ğŸ“ˆ Training & Tuning

- No manual intervention is required between steps

- Hyperparameters flow automatically from tuning to training### HybridVAE Model

- Experiments are fully reproducible from the same `.env` configuration

### Training Curves

---

## Dataset Format

## Implementation Details

<p align="center">

### Data Preprocessing (`src/preprocessing/`)

  <img src="assets/all_beauty_training.png" width="48%" alt="All_Beauty Training"/>```

- **cleaning.py**: Loads JSONL Amazon review data, filters users and items with fewer than 5 interactions, and converts ratings â‰¥4 to positive interactions.

- **dataset.py**: Creates temporal train/validation/test splits (70/15/15) based on review timestamps. Builds the user-item interaction matrix and saves mappings.  <img src="assets/appliances_training.png" width="48%" alt="Appliances Training"/>

- **embeddings.py**: Generates item embeddings by concatenating the product title with aggregated review text, then encoding with Sentence-BERT.

</p>Input: User interaction vector (n_items,)The system expects Amazon Reviews data in JSONL format with the following fields:

### Model (`src/ml/model.py`)

<p align="center"><em>Training loss curves and metrics over epochs</em></p>

The `HybridVAE` class implements:

- Configurable encoder with LayerNorm and GELU activations         â”‚- `user_id`: Unique user identifier

- Reparameterization trick for sampling from the latent distribution

- Projection layer mapping latent space to embedding space### Grid Search Hyperparameter Tuning

- KL annealing schedule to prevent posterior collapse

- Frozen embedding decoder         â–¼- `asin`: Amazon Standard Identification Number (item ID)



### Training (`src/ml/train.py`)<p align="center">



Training features include:  <img src="assets/all_beauty_gridsearch.png" width="60%" alt="Grid Search Heatmap"/>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”- `rating`: Rating score (1-5)

- Adam optimizer with gradient clipping (max norm 5.0)

- Early stopping based on validation loss</p>

- Checkpoint saving at each epoch

- Training history export to JSON<p align="center"><em>Hyperparameter impact on NDCG@10 (All_Beauty)</em></p>â”‚  Encoder                            â”‚- `title`: Product title



### Hyperparameter Tuning (`src/ml/tune.py`)



Grid search over:### Latent Space Visualizationâ”‚  Linear(n_items â†’ 512) + LayerNorm  â”‚- `text`: Review text

- `latent_dim`: [64, 128]

- `hidden_dims`: [[256], [512]]

- `dropout`: [0.3, 0.5]

- `beta`: [0.1, 0.2]<p align="center">â”‚  GELU + Dropout(0.3)                â”‚- `timestamp`: Review timestamp

- `learning_rate`: [1e-3]

  <img src="assets/all_beauty_latent_tsne.png" width="48%" alt="All_Beauty Latent Space"/>

Each configuration is trained for 5 epochs with early stopping (patience=2). The best configuration is selected based on NDCG@10 on the validation set.

  <img src="assets/appliances_latent_tsne.png" width="48%" alt="Appliances Latent Space"/>â”‚  Linear(512 â†’ Î¼, Ïƒ)                 â”‚

### Evaluation (`src/ml/evaluate.py`)

</p>

Evaluation uses the standard negative sampling protocol:

- For each test interaction, sample 99 random negative items<p align="center"><em>t-SNE visualization of learned user embeddings</em></p>â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜## Usage

- Rank the test item among the 100 candidates

- Compute Recall@K, NDCG@K, and Hit Ratio@K for K âˆˆ {5, 10, 20}



### Baselines (`src/ml/baseline.py`)## ğŸ“ Project Structure         â”‚



| Model | Implementation |

|:------|:---------------|

| Popularity | Item frequency ranking |```         â–¼ Reparameterization: z = Î¼ + ÏƒÂ·Îµ### 1. Data Preprocessing

| Item-KNN | Cosine similarity on interaction vectors (k=50) |

| SVD | Matrix factorization via Surprise library (100 factors) |recommendation_system/

| Mult-VAE | Multinomial VAE following Liang et al. (2018) |

| LightGCN | 3-layer graph convolution with BPR loss |â”œâ”€â”€ data/                    # Dataset storage         â”‚```bash



All baselines use the same train/test splits and evaluation protocol for fair comparison.â”‚   â”œâ”€â”€ All_Beauty.jsonl     # Raw Amazon reviews



---â”‚   â”œâ”€â”€ train.csv            # Training splitâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”python src/preprocessing/cleaning.py --input data/reviews.jsonl --output data/cleaned_reviews.jsonl



## Installationâ”‚   â”œâ”€â”€ val.csv              # Validation split



```bashâ”‚   â””â”€â”€ test.csv             # Test splitâ”‚  Decoder (Frozen SBERT Embeddings)  â”‚```

git clone https://github.com/Aymane-Nouhail/Recommendation-System.git

cd Recommendation-Systemâ”œâ”€â”€ embeddings/              # Pre-computed SBERT embeddings

pip install -r requirements.txt

```â”‚   â”œâ”€â”€ item_embeddings.npyâ”‚  Projection: Linear(latent â†’ 384)   â”‚



Requirements include PyTorch, sentence-transformers, scipy, scikit-learn, and surprise.â”‚   â””â”€â”€ item_embeddings_mappings.pkl



---â”œâ”€â”€ models/                  # Saved checkpoints & resultsâ”‚  logits = Embeddings @ z            â”‚### 2. Build Dataset



## Usageâ”‚   â”œâ”€â”€ best_model.pth



### Full Pipelineâ”‚   â”œâ”€â”€ grid_search_results.jsonâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜```bash



```bashâ”‚   â”œâ”€â”€ evaluation_results.json

# Configure dataset in .env

echo "RAW_DATA_FILE=All_Beauty.jsonl" > .envâ”‚   â””â”€â”€ baseline_results.json         â”‚python src/preprocessing/dataset.py --input data/cleaned_reviews.jsonl --output data/



# Run everythingâ”œâ”€â”€ assets/                  # README images

make all

```â”œâ”€â”€ src/         â–¼```



### Individual Stepsâ”‚   â”œâ”€â”€ preprocessing/



```bashâ”‚   â”‚   â”œâ”€â”€ cleaning.py      # Data loading & filteringOutput: Reconstruction scores (n_items,)

make preprocess    # Clean data, create splits, compute embeddings

make tune          # Grid search for best hyperparametersâ”‚   â”‚   â”œâ”€â”€ dataset.py       # Train/val/test splits

make train-best    # Train with best configuration

make evaluate      # Evaluate on test setâ”‚   â”‚   â””â”€â”€ embeddings.py    # SBERT embedding generation```### 3. Compute Item Embeddings

make baseline      # Run all baseline models

make visualize     # Generate plotsâ”‚   â”œâ”€â”€ ml/

```

â”‚   â”‚   â”œâ”€â”€ model.py         # HybridVAE architecture```bash

### Configuration

â”‚   â”‚   â”œâ”€â”€ train.py         # Training loop

Edit `.env` to change the dataset or model parameters:

â”‚   â”‚   â”œâ”€â”€ evaluate.py      # Metrics (Recall, NDCG, HR)**Key Components:**python src/preprocessing/embeddings.py --input data/cleaned_reviews.jsonl --output embeddings/item_embeddings.npy

```bash

RAW_DATA_FILE=All_Beauty.jsonlâ”‚   â”‚   â”œâ”€â”€ tune.py          # Grid search hyperparameter tuning

EMBEDDING_MODEL=all-MiniLM-L6-v2

BATCH_SIZE=64â”‚   â”‚   â”œâ”€â”€ baseline.py      # Baseline models- **SBERT Embeddings**: `all-MiniLM-L6-v2` (384 dimensions) - frozen during training```

EPOCHS=20

```â”‚   â”‚   â””â”€â”€ visualize.py     # Result visualization



---â”‚   â””â”€â”€ api/- **Latent Space**: 128 dimensions (found via grid search)



## Methodologyâ”‚       â”œâ”€â”€ server.py        # FastAPI inference server



### Training Curvesâ”‚       â””â”€â”€ schemas.py       # Pydantic models- **Loss**: Reconstruction (BCE) + Î² Ã— KL Divergence with annealing### 4. Train the Model



<p align="center">â”œâ”€â”€ backups/                 # Experiment backups

  <img src="assets/all_beauty_training.png" width="45%" />

  &nbsp;&nbsp;â”œâ”€â”€ Makefile                 # Pipeline automation- **Regularization**: Dropout, LayerNorm, gradient clipping```bash

  <img src="assets/appliances_training.png" width="45%" />

</p>â”œâ”€â”€ requirements.txt



### Hyperparameter Searchâ””â”€â”€ README.mdpython src/ml/train.py --data data/ --embeddings embeddings/item_embeddings.npy --output models/



<p align="center">```

  <img src="assets/all_beauty_gridsearch.png" width="60%" />

</p>## ğŸ“ Project Structure```



<p align="center">## ğŸš€ Quick Start

  <sub>Impact of hyperparameters on NDCG@10 (All_Beauty dataset).</sub>

</p>



### Latent Space### Installation



<p align="center">```### 5. Evaluate the Model

  <img src="assets/all_beauty_latent_tsne.png" width="45%" />

  &nbsp;&nbsp;```bash

  <img src="assets/appliances_latent_tsne.png" width="45%" />

</p>git clone https://github.com/Aymane-Nouhail/Recommendation-System.gitrecommendation_system/```bash



<p align="center">cd recommendation_system

  <sub>t-SNE visualization of learned user representations.</sub>

</p>pip install -r requirements.txtâ”œâ”€â”€ data/                    # Dataset storagepython src/ml/evaluate.py --model models/best_model.pth --data data/ --embeddings embeddings/item_embeddings.npy



---```



## Project Structureâ”‚   â”œâ”€â”€ All_Beauty.jsonl     # Raw Amazon reviews```



```### Run Full Pipeline

recommendation_system/

â”œâ”€â”€ data/                    # Raw and processed dataâ”‚   â”œâ”€â”€ train.csv            # Training split

â”œâ”€â”€ embeddings/              # Pre-computed item embeddings

â”œâ”€â”€ models/                  # Checkpoints and results```bash

â”‚   â””â”€â”€ figures/             # Generated plots

â”œâ”€â”€ assets/                  # README images# Run everything: preprocess â†’ tune â†’ train â†’ evaluate â†’ baselines â†’ visualizeâ”‚   â”œâ”€â”€ val.csv              # Validation split### 6. Start the API Server

â”œâ”€â”€ src/

â”‚   â”œâ”€â”€ preprocessing/make all

â”‚   â”‚   â”œâ”€â”€ cleaning.py

â”‚   â”‚   â”œâ”€â”€ dataset.py```â”‚   â””â”€â”€ test.csv             # Test split```bash

â”‚   â”‚   â””â”€â”€ embeddings.py

â”‚   â”œâ”€â”€ ml/

â”‚   â”‚   â”œâ”€â”€ model.py         # HybridVAE implementation

â”‚   â”‚   â”œâ”€â”€ train.py### Individual Stepsâ”œâ”€â”€ embeddings/              # Pre-computed SBERT embeddingspython src/api/server.py

â”‚   â”‚   â”œâ”€â”€ evaluate.py

â”‚   â”‚   â”œâ”€â”€ tune.py          # Grid search

â”‚   â”‚   â”œâ”€â”€ baseline.py      # Baseline models

â”‚   â”‚   â””â”€â”€ visualize.py```bashâ”‚   â”œâ”€â”€ item_embeddings.npy```

â”‚   â””â”€â”€ api/

â”‚       â”œâ”€â”€ server.py        # FastAPI inference# 1. Preprocess data (clean, split, compute embeddings)

â”‚       â””â”€â”€ schemas.py

â”œâ”€â”€ Makefile                 # Pipeline automationmake preprocessâ”‚   â””â”€â”€ item_embeddings_mappings.pkl

â”œâ”€â”€ requirements.txt

â””â”€â”€ README.md

```

# 2. Hyperparameter tuning (grid search)â”œâ”€â”€ models/                  # Saved checkpoints & resultsThe API will be available at `http://localhost:8000`

---

make tune

## References

â”‚   â”œâ”€â”€ best_model.pth

- Kingma, D. P., & Welling, M. (2014). Auto-Encoding Variational Bayes. *ICLR*.

- Liang, D., et al. (2018). Variational Autoencoders for Collaborative Filtering. *WWW*.# 3. Train with best hyperparameters

- He, X., et al. (2020). LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation. *SIGIR*.

- Reimers, N., & Gurevych, I. (2019). Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. *EMNLP*.make train-bestâ”‚   â”œâ”€â”€ grid_search_results.json## API Endpoints



---



## License# 4. Evaluate HybridVAEâ”‚   â”œâ”€â”€ evaluation_results.json



MIT Licensemake evaluate


â”‚   â””â”€â”€ baseline_results.json### POST `/recommend`

# 5. Run baseline models

make baselineâ”œâ”€â”€ src/Get recommendations for a user.



# 6. Generate visualizationsâ”‚   â”œâ”€â”€ preprocessing/

make visualize

```â”‚   â”‚   â”œâ”€â”€ cleaning.py      # Data loading & filtering**Request:**



## ğŸ”¬ Methodologyâ”‚   â”‚   â”œâ”€â”€ dataset.py       # Train/val/test splits```json



### Data Preprocessingâ”‚   â”‚   â””â”€â”€ embeddings.py    # SBERT embedding generation{

1. **Filtering**: Remove users with <5 interactions, items with <5 interactions

2. **Binary ratings**: Convert ratings â‰¥4 to positive (1), else negative (0)â”‚   â”œâ”€â”€ ml/    "user_id": "user123",

3. **Temporal split**: Train (70%) / Validation (15%) / Test (15%) by timestamp

4. **Embeddings**: SBERT (`all-MiniLM-L6-v2`) on item title + aggregated reviewsâ”‚   â”‚   â”œâ”€â”€ model.py         # HybridVAE architecture    "top_k": 10



### Hyperparameter Tuning (Grid Search)â”‚   â”‚   â”œâ”€â”€ train.py         # Training loop}



| Parameter | Search Space | Best (All_Beauty) |â”‚   â”‚   â”œâ”€â”€ evaluate.py      # Metrics (Recall, NDCG, HR)```

|-----------|--------------|-------------------|

| latent_dim | [64, 128] | 128 |â”‚   â”‚   â”œâ”€â”€ tune.py          # Grid search hyperparameter tuning

| hidden_dims | [[256], [512]] | [512] |

| dropout | [0.3, 0.5] | 0.3 |â”‚   â”‚   â”œâ”€â”€ baseline.py      # Baseline models**Response:**

| beta | [0.1, 0.2] | 0.2 |

| learning_rate | [1e-3] | 1e-3 |â”‚   â”‚   â””â”€â”€ visualize.py     # Result visualization```json



**Tuning Protocol:**â”‚   â””â”€â”€ api/{

- 5 epochs per config with early stopping (patience=2)

- Validation metric: NDCG@10â”‚       â”œâ”€â”€ server.py        # FastAPI inference server    "user_id": "user123",

- Best config selected for final training (20 epochs)

â”‚       â””â”€â”€ schemas.py       # Pydantic models    "recommendations": [

### Evaluation Protocol

- **Negative Sampling**: 99 random negatives per test itemâ”œâ”€â”€ backups/                 # Experiment backups        {

- **Metrics**: Recall@K, NDCG@K, Hit Ratio@K for K âˆˆ {5, 10, 20}

- **Fair comparison**: Same splits and protocol for all modelsâ”œâ”€â”€ Makefile                 # Pipeline automation            "item_id": "B001234567",



### Baseline Modelsâ”œâ”€â”€ requirements.txt            "score": 0.95

| Model | Description |

|-------|-------------|â””â”€â”€ README.md        },

| Popularity | Rank by item frequency |

| Item-KNN | Cosine similarity on interaction vectors |```        ...

| SVD | Matrix factorization (surprise library) |

| Mult-VAE | Multinomial VAE (Liang et al., 2018) |    ]

| LightGCN | Graph convolution + BPR loss (He et al., 2020) |

## ğŸš€ Quick Start}

## âš™ï¸ Configuration

```

Environment variables (`.env`):

```bash### Installation

RAW_DATA_FILE=All_Beauty.jsonl

EMBEDDING_MODEL=all-MiniLM-L6-v2## Model Architecture

BATCH_SIZE=64

EPOCHS=20```bash

LATENT_DIM=128

```git clone https://github.com/Aymane-Nouhail/Recommendation-System.gitThe Hybrid VAE combines:



## ğŸ”Œ API Usagecd recommendation_system



Start the server:pip install -r requirements.txt1. **Collaborative Filtering**: User-item interaction patterns

```bash

make run-api```2. **Content-Based Filtering**: SBERT embeddings of item text (title + review)

# or

python src/api/server.py

```

### Run Full Pipeline### VAE Components:

Get recommendations:

```bash- **Encoder**: Maps user interaction vector to latent space (Î¼, Ïƒ)

curl -X POST http://localhost:8000/recommend \

  -H "Content-Type: application/json" \```bash- **Reparameterization**: z = Î¼ + Ïƒ * Îµ (where Îµ ~ N(0,1))

  -d '{"user_id": "A123456", "top_k": 10}'

```# Run everything: preprocess â†’ tune â†’ train â†’ evaluate â†’ baselines â†’ visualize- **Decoder**: Uses item embeddings as decoder weights: logits = E @ z



## ğŸ“š Referencesmake all



- Kingma & Welling (2014). Auto-Encoding Variational Bayes```### Loss Function:

- Liang et al. (2018). Variational Autoencoders for Collaborative Filtering

- He et al. (2020). LightGCN: Simplifying and Powering Graph Convolution Network```

- Reimers & Gurevych (2019). Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks

### Individual StepsLoss = Reconstruction Loss + Î² * KL Divergence

## ğŸ“ License

```

MIT License

```bash

# 1. Preprocess data (clean, split, compute embeddings)## Evaluation Metrics

make preprocess

- **Recall@10**: Fraction of relevant items in top-10 recommendations

# 2. Hyperparameter tuning (grid search)- **NDCG@10**: Normalized Discounted Cumulative Gain at rank 10

make tune

## Configuration

# 3. Train with best hyperparameters

make train-bestKey hyperparameters can be adjusted in the training script:

- `latent_dim`: Dimensionality of latent space (default: 200)

# 4. Evaluate HybridVAE- `beta`: KL divergence weight (default: 0.2)

make evaluate- `learning_rate`: Adam optimizer learning rate (default: 0.001)

- `batch_size`: Mini-batch size (default: 512)

# 5. Run baseline models- `epochs`: Number of training epochs (default: 100)

make baseline

## License

# 6. Generate visualizations

make visualizeMIT License
```

## ğŸ”¬ Methodology

### Data Preprocessing
1. **Filtering**: Remove users with <5 interactions, items with <5 interactions
2. **Binary ratings**: Convert ratings â‰¥4 to positive (1), else negative (0)
3. **Temporal split**: Train (70%) / Validation (15%) / Test (15%) by timestamp
4. **Embeddings**: SBERT (`all-MiniLM-L6-v2`) on item title + aggregated reviews

### Hyperparameter Tuning (Grid Search)

| Parameter | Search Space | Best (All_Beauty) |
|-----------|--------------|-------------------|
| latent_dim | [64, 128] | 128 |
| hidden_dims | [[256], [512]] | [512] |
| dropout | [0.3, 0.5] | 0.3 |
| beta | [0.1, 0.2] | 0.2 |
| learning_rate | [1e-3] | 1e-3 |

**Tuning Protocol:**
- 5 epochs per config with early stopping (patience=2)
- Validation metric: NDCG@10
- Best config selected for final training (20 epochs)

### Evaluation Protocol
- **Negative Sampling**: 99 random negatives per test item
- **Metrics**: Recall@K, NDCG@K, Hit Ratio@K for K âˆˆ {5, 10, 20}
- **Fair comparison**: Same splits and protocol for all models

### Baseline Models
| Model | Description |
|-------|-------------|
| Popularity | Rank by item frequency |
| Item-KNN | Cosine similarity on interaction vectors |
| SVD | Matrix factorization (surprise library) |
| Mult-VAE | Multinomial VAE (Liang et al., 2018) |
| LightGCN | Graph convolution + BPR loss (He et al., 2020) |

## âš™ï¸ Configuration

Environment variables (`.env`):
```bash
RAW_DATA_FILE=All_Beauty.jsonl
EMBEDDING_MODEL=all-MiniLM-L6-v2
BATCH_SIZE=64
EPOCHS=20
LATENT_DIM=128
```

## ğŸ“Š Visualizations

The pipeline generates:
- `models/figures/training_curves.png` - Loss curves
- `models/figures/baseline_comparison.png` - Model comparison bar charts
- `models/figures/tuning_results.png` - Grid search heatmaps

## ğŸ”Œ API Usage

Start the server:
```bash
make run-api
# or
python src/api/server.py
```

Get recommendations:
```bash
curl -X POST http://localhost:8000/recommend \
  -H "Content-Type: application/json" \
  -d '{"user_id": "A123456", "top_k": 10}'
```

## ğŸ“š References

- Kingma & Welling (2014). Auto-Encoding Variational Bayes
- Liang et al. (2018). Variational Autoencoders for Collaborative Filtering
- He et al. (2020). LightGCN: Simplifying and Powering Graph Convolution Network
- Reimers & Gurevych (2019). Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks

## ğŸ“ License

MIT License
